{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-09T17:52:50.871986Z",
     "start_time": "2023-12-09T17:52:48.195461Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.layers as layers\n",
    "import scipy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "img_size=(240, 320)\n",
    "img_channels = 3\n",
    "batch_size=32"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T17:52:50.876603Z",
     "start_time": "2023-12-09T17:52:50.872583Z"
    }
   },
   "id": "c902b5ffcc0be518"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "model_name = 'simple_covnet_model.tf'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T17:52:50.877428Z",
     "start_time": "2023-12-09T17:52:50.874930Z"
    }
   },
   "id": "8119c7b1f46208d2"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train_size = 1000\n",
    "validation_size = 500\n",
    "test_size = 500"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T17:52:50.881943Z",
     "start_time": "2023-12-09T17:52:50.877906Z"
    }
   },
   "id": "57836484d49b556a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the DataSet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a698c6c6147e952e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dataset citation:\n",
    "- From Semi-Supervised to Transfer Counting of Crowds\n",
    "C. C. Loy, S. Gong, and T. Xiang\n",
    "in Proceedings of IEEE International Conference on Computer Vision, pp. 2256-2263, 2013 (ICCV)\n",
    "- Cumulative Attribute Space for Age and Crowd Density Estimation\n",
    "K. Chen, S. Gong, T. Xiang, and C. C. Loy\n",
    "in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pp. 2467-2474, 2013 (CVPR, Oral)\n",
    "- Crowd Counting and Profiling: Methodology and Evaluation\n",
    "C. C. Loy, K. Chen, S. Gong, T. Xiang\n",
    "in S. Ali, K. Nishino, D. Manocha, and M. Shah (Eds.), Modeling, Simulation and Visual Analysis of Crowds, Springer, vol. 11, pp. 347-382, 2013\n",
    "- Feature Mining for Localised Crowd Counting\n",
    "K. Chen, C. C. Loy, S. Gong, and T. Xiang\n",
    "British Machine Vision Conference, 2012 (BMVC)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3148fc7158b3013"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "dataset_path = '/Users/olove/Library/CloudStorage/OneDrive-Personal/AI datasets/CrowdCounter'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T17:52:50.882498Z",
     "start_time": "2023-12-09T17:52:50.879969Z"
    }
   },
   "id": "2a61029b86dd4523"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "      count      image_name\n0        35  seq_000001.jpg\n1        41  seq_000002.jpg\n2        41  seq_000003.jpg\n3        44  seq_000004.jpg\n4        41  seq_000005.jpg\n...     ...             ...\n1995     27  seq_001996.jpg\n1996     27  seq_001997.jpg\n1997     25  seq_001998.jpg\n1998     26  seq_001999.jpg\n1999     26  seq_002000.jpg\n\n[2000 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>image_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>35</td>\n      <td>seq_000001.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>41</td>\n      <td>seq_000002.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>41</td>\n      <td>seq_000003.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>44</td>\n      <td>seq_000004.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>41</td>\n      <td>seq_000005.jpg</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1995</th>\n      <td>27</td>\n      <td>seq_001996.jpg</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>27</td>\n      <td>seq_001997.jpg</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>25</td>\n      <td>seq_001998.jpg</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>26</td>\n      <td>seq_001999.jpg</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>26</td>\n      <td>seq_002000.jpg</td>\n    </tr>\n  </tbody>\n</table>\n<p>2000 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_df = pd.read_csv(dataset_path + '/labels.csv')\n",
    "labels_df['image_name'] = labels_df['id'].map('seq_{:06d}.jpg'.format)\n",
    "labels_df.drop(\"id\", axis=1,inplace=True)\n",
    "display(labels_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T17:52:50.894606Z",
     "start_time": "2023-12-09T17:52:50.882640Z"
    }
   },
   "id": "d2dae3a51c2c8948"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e7301d83edda066c"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "      count      image_name\n0        32  seq_000355.jpg\n1        40  seq_000741.jpg\n2        31  seq_001697.jpg\n3        27  seq_000512.jpg\n4        42  seq_001715.jpg\n...     ...             ...\n1995     24  seq_000606.jpg\n1996     30  seq_001966.jpg\n1997     24  seq_001155.jpg\n1998     43  seq_001652.jpg\n1999     49  seq_000764.jpg\n\n[2000 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>image_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>32</td>\n      <td>seq_000355.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>40</td>\n      <td>seq_000741.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>31</td>\n      <td>seq_001697.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>27</td>\n      <td>seq_000512.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>42</td>\n      <td>seq_001715.jpg</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1995</th>\n      <td>24</td>\n      <td>seq_000606.jpg</td>\n    </tr>\n    <tr>\n      <th>1996</th>\n      <td>30</td>\n      <td>seq_001966.jpg</td>\n    </tr>\n    <tr>\n      <th>1997</th>\n      <td>24</td>\n      <td>seq_001155.jpg</td>\n    </tr>\n    <tr>\n      <th>1998</th>\n      <td>43</td>\n      <td>seq_001652.jpg</td>\n    </tr>\n    <tr>\n      <th>1999</th>\n      <td>49</td>\n      <td>seq_000764.jpg</td>\n    </tr>\n  </tbody>\n</table>\n<p>2000 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_df = labels_df.sample(frac=1).reset_index(drop=True)\n",
    "display(labels_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T17:52:50.903922Z",
     "start_time": "2023-12-09T17:52:50.892125Z"
    }
   },
   "id": "1d34f79116ad0d0a"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "if (train_size+validation_size+test_size) != len(labels_df):\n",
    "    print('Dataset size is different from specified class sizes')\n",
    "    exit(1)\n",
    "\n",
    "training_df = labels_df[:train_size]\n",
    "validation_df = labels_df[train_size:train_size+validation_size].reset_index(drop=True)\n",
    "test_df = labels_df[train_size+validation_size:].reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T17:52:50.904325Z",
     "start_time": "2023-12-09T17:52:50.897508Z"
    }
   },
   "id": "68ffbd06d94921cd"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T17:52:50.925243Z",
     "start_time": "2023-12-09T17:52:50.899638Z"
    }
   },
   "id": "5863c3b67def8ae2"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 validated image filenames.\n",
      "Found 500 validated image filenames.\n",
      "Found 500 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    \n",
    ")\n",
    "\n",
    "x_col_name = 'image_name'\n",
    "y_col_name = 'count'\n",
    "\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    training_df, \n",
    "    dataset_path + '/frames/frames/',\n",
    "    x_col=x_col_name,\n",
    "    y_col=y_col_name,\n",
    "    class_mode='raw',\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_dataframe(\n",
    "    validation_df,\n",
    "    dataset_path + '/frames/frames/',\n",
    "    x_col=x_col_name,\n",
    "    y_col=y_col_name,\n",
    "    class_mode='raw',\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "test_generator = datagen.flow_from_dataframe(\n",
    "    test_df,\n",
    "    dataset_path + '/frames/frames/',\n",
    "    x_col=x_col_name,\n",
    "    y_col=y_col_name,\n",
    "    class_mode='raw',\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T17:52:50.955709Z",
     "start_time": "2023-12-09T17:52:50.903738Z"
    }
   },
   "id": "d64c64301c8e55a1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73cb3d24c1555a7b"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape= img_size + (img_channels,))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T17:52:50.956128Z",
     "start_time": "2023-12-09T17:52:50.918850Z"
    }
   },
   "id": "d448c07728ee1f3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Custom Simple Covnet\n",
    "\n",
    "Downsizing using strides instead of MaxPolling in order to conserve location data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bce077d02d7424f9"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-09 17:52:50.928000: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2023-12-09 17:52:50.928023: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2023-12-09 17:52:50.928028: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2023-12-09 17:52:50.928060: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-12-09 17:52:50.928073: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "#x = layers.Rescaling(1./255)(inputs)\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, strides=2, activation=\"relu\")(inputs)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, strides=2, activation=\"relu\")(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, strides=2, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "# x = layers.Dropout(0.5)(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T17:52:51.065924Z",
     "start_time": "2023-12-09T17:52:50.927618Z"
    }
   },
   "id": "8e87c1bf0eed10e8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Output Layer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35ca745a6e1390d9"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "outputs = layers.Dense(128, activation=\"relu\")(x)\n",
    "outputs = layers.Dropout(0.5)(outputs)\n",
    "outputs = layers.Dense(1)(outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T17:52:51.113139Z",
     "start_time": "2023-12-09T17:52:51.065039Z"
    }
   },
   "id": "2d3e86c32901040b"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T17:52:51.123710Z",
     "start_time": "2023-12-09T17:52:51.114889Z"
    }
   },
   "id": "6e5f9a14eaffc43a"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 240, 320, 3)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 119, 159, 32)      896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 59, 79, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 29, 39, 128)       73856     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 144768)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               18530432  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 18623809 (71.04 MB)\n",
      "Trainable params: 18623809 (71.04 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T17:52:51.133983Z",
     "start_time": "2023-12-09T17:52:51.120715Z"
    }
   },
   "id": "e5dd3df119d8e3d9"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-09T17:52:51.136439Z"
    }
   },
   "id": "b6eb6db3c2d9a8ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2300381ac22fbd71"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "# TODO: Try mae vs accuracy. mae should be better since we are adjusting it to get closer to the actual value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T17:52:51.147920Z",
     "start_time": "2023-12-09T17:52:51.137748Z"
    }
   },
   "id": "b6a1a37a09407565"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "#    keras.callbacks.EarlyStopping(\n",
    "#        monitor=\"val_loss\", patience=4\n",
    "#    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=model_name,\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True\n",
    "    ),\n",
    "    keras.callbacks.TensorBoard()\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T17:57:02.620318Z",
     "start_time": "2023-12-09T17:57:02.615480Z"
    }
   },
   "id": "b63b6dd1a1758bf6"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "Wsave = model.get_weights()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T17:52:51.181716Z",
     "start_time": "2023-12-09T17:52:51.148802Z"
    }
   },
   "id": "6c1fb86c900a890f"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "32/32 [==============================] - ETA: 0s - loss: 180.8535 - mae: 10.4068INFO:tensorflow:Assets written to: simple_covnet_model.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: simple_covnet_model.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 5s 152ms/step - loss: 180.8535 - mae: 10.4068 - val_loss: 107.0750 - val_mae: 8.6869\n",
      "Epoch 2/60\n",
      "32/32 [==============================] - ETA: 0s - loss: 121.0344 - mae: 8.7333INFO:tensorflow:Assets written to: simple_covnet_model.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: simple_covnet_model.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 4s 138ms/step - loss: 121.0344 - mae: 8.7333 - val_loss: 59.5206 - val_mae: 6.1519\n",
      "Epoch 3/60\n",
      "32/32 [==============================] - ETA: 0s - loss: 121.2174 - mae: 8.7683INFO:tensorflow:Assets written to: simple_covnet_model.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: simple_covnet_model.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 4s 137ms/step - loss: 121.2174 - mae: 8.7683 - val_loss: 54.8353 - val_mae: 5.8427\n",
      "Epoch 4/60\n",
      "32/32 [==============================] - ETA: 0s - loss: 99.1314 - mae: 7.9491 INFO:tensorflow:Assets written to: simple_covnet_model.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: simple_covnet_model.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 4s 134ms/step - loss: 99.1314 - mae: 7.9491 - val_loss: 52.6107 - val_mae: 5.7055\n",
      "Epoch 5/60\n",
      "32/32 [==============================] - ETA: 0s - loss: 85.2915 - mae: 7.5355INFO:tensorflow:Assets written to: simple_covnet_model.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: simple_covnet_model.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 4s 133ms/step - loss: 85.2915 - mae: 7.5355 - val_loss: 49.0757 - val_mae: 5.5588\n",
      "Epoch 6/60\n",
      "32/32 [==============================] - ETA: 0s - loss: 60.7687 - mae: 6.3376INFO:tensorflow:Assets written to: simple_covnet_model.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: simple_covnet_model.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 4s 132ms/step - loss: 60.7687 - mae: 6.3376 - val_loss: 45.4896 - val_mae: 5.3221\n",
      "Epoch 7/60\n",
      "32/32 [==============================] - ETA: 0s - loss: 50.7838 - mae: 5.7701INFO:tensorflow:Assets written to: simple_covnet_model.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: simple_covnet_model.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 4s 132ms/step - loss: 50.7838 - mae: 5.7701 - val_loss: 41.1479 - val_mae: 5.1144\n",
      "Epoch 8/60\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 49.9577 - mae: 5.6913 - val_loss: 102.9726 - val_mae: 8.8628\n",
      "Epoch 9/60\n",
      "32/32 [==============================] - 4s 113ms/step - loss: 45.5119 - mae: 5.4169 - val_loss: 76.8898 - val_mae: 7.5962\n",
      "Epoch 10/60\n",
      "32/32 [==============================] - ETA: 0s - loss: 39.0798 - mae: 4.9851INFO:tensorflow:Assets written to: simple_covnet_model.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: simple_covnet_model.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 4s 134ms/step - loss: 39.0798 - mae: 4.9851 - val_loss: 30.2443 - val_mae: 4.4250\n",
      "Epoch 11/60\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 62.9001 - mae: 6.3368 - val_loss: 140.3793 - val_mae: 10.8799\n",
      "Epoch 12/60\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 36.2871 - mae: 4.7128 - val_loss: 95.7077 - val_mae: 8.8840\n",
      "Epoch 13/60\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 25.5212 - mae: 3.9663 - val_loss: 73.9679 - val_mae: 7.7252\n",
      "Epoch 14/60\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 28.9229 - mae: 4.2908 - val_loss: 163.0115 - val_mae: 12.0500\n",
      "Epoch 15/60\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 25.3692 - mae: 3.9723 - val_loss: 78.1761 - val_mae: 8.0039\n",
      "Epoch 16/60\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 22.8997 - mae: 3.7414 - val_loss: 69.4288 - val_mae: 7.5385\n",
      "Epoch 17/60\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 20.9125 - mae: 3.6216 - val_loss: 125.7161 - val_mae: 10.5544\n",
      "Epoch 18/60\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 18.8961 - mae: 3.4299 - val_loss: 80.6637 - val_mae: 8.2601\n",
      "Epoch 19/60\n",
      "32/32 [==============================] - 4s 114ms/step - loss: 19.6609 - mae: 3.5009 - val_loss: 74.2706 - val_mae: 7.8904\n",
      "Epoch 20/60\n",
      "32/32 [==============================] - 4s 113ms/step - loss: 19.4281 - mae: 3.4424 - val_loss: 86.1704 - val_mae: 8.5933\n",
      "Epoch 21/60\n",
      "32/32 [==============================] - 4s 114ms/step - loss: 19.5530 - mae: 3.5455 - val_loss: 93.8019 - val_mae: 9.0019\n",
      "Epoch 22/60\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 16.5416 - mae: 3.2185 - val_loss: 85.8498 - val_mae: 8.6212\n",
      "Epoch 23/60\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 15.6836 - mae: 3.1147 - val_loss: 47.8396 - val_mae: 6.1838\n",
      "Epoch 24/60\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 15.4825 - mae: 3.0647 - val_loss: 55.4276 - val_mae: 6.7436\n",
      "Epoch 25/60\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 15.9650 - mae: 3.1401 - val_loss: 87.5223 - val_mae: 8.7616\n",
      "Epoch 26/60\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 13.9042 - mae: 2.9196 - val_loss: 74.4984 - val_mae: 8.0103\n",
      "Epoch 27/60\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 16.6318 - mae: 3.2155 - val_loss: 59.1252 - val_mae: 7.0374\n",
      "Epoch 28/60\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 14.0490 - mae: 2.9525 - val_loss: 39.7572 - val_mae: 5.5780\n",
      "Epoch 29/60\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 14.8930 - mae: 3.0760 - val_loss: 51.7580 - val_mae: 6.5505\n",
      "Epoch 30/60\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 13.0768 - mae: 2.8346 - val_loss: 83.6502 - val_mae: 8.5903\n",
      "Epoch 31/60\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 29.6266 - mae: 4.3301 - val_loss: 76.0415 - val_mae: 8.1158\n",
      "Epoch 32/60\n",
      "32/32 [==============================] - 4s 114ms/step - loss: 20.3047 - mae: 3.5860 - val_loss: 134.2930 - val_mae: 11.0956\n",
      "Epoch 33/60\n",
      "32/32 [==============================] - 4s 114ms/step - loss: 17.0292 - mae: 3.2515 - val_loss: 31.6273 - val_mae: 4.8412\n",
      "Epoch 34/60\n",
      "32/32 [==============================] - 4s 114ms/step - loss: 13.8978 - mae: 3.0083 - val_loss: 122.5747 - val_mae: 10.6387\n",
      "Epoch 35/60\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 13.1915 - mae: 2.8770 - val_loss: 73.5305 - val_mae: 8.0087\n",
      "Epoch 36/60\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 14.5125 - mae: 3.0216 - val_loss: 49.9777 - val_mae: 6.4612\n",
      "Epoch 37/60\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 12.0480 - mae: 2.6620 - val_loss: 65.2389 - val_mae: 7.5083\n",
      "Epoch 38/60\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 11.6410 - mae: 2.7140 - val_loss: 59.3462 - val_mae: 7.1201\n",
      "Epoch 39/60\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 12.2535 - mae: 2.7322 - val_loss: 76.1817 - val_mae: 8.1975\n",
      "Epoch 40/60\n",
      "32/32 [==============================] - ETA: 0s - loss: 13.0812 - mae: 2.8826INFO:tensorflow:Assets written to: simple_covnet_model.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: simple_covnet_model.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 4s 137ms/step - loss: 13.0812 - mae: 2.8826 - val_loss: 25.0188 - val_mae: 4.3009\n",
      "Epoch 41/60\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 12.9826 - mae: 2.8280 - val_loss: 32.3796 - val_mae: 4.9912\n",
      "Epoch 42/60\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 11.4923 - mae: 2.6690 - val_loss: 40.2085 - val_mae: 5.6846\n",
      "Epoch 43/60\n",
      "32/32 [==============================] - 4s 114ms/step - loss: 11.4921 - mae: 2.6487 - val_loss: 54.7785 - val_mae: 6.8703\n",
      "Epoch 44/60\n",
      "32/32 [==============================] - 4s 114ms/step - loss: 11.4196 - mae: 2.6165 - val_loss: 49.3767 - val_mae: 6.4161\n",
      "Epoch 45/60\n",
      "32/32 [==============================] - ETA: 0s - loss: 14.0084 - mae: 2.9479INFO:tensorflow:Assets written to: simple_covnet_model.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: simple_covnet_model.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 4s 136ms/step - loss: 14.0084 - mae: 2.9479 - val_loss: 19.9988 - val_mae: 3.7911\n",
      "Epoch 46/60\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 12.6742 - mae: 2.8207 - val_loss: 57.3349 - val_mae: 6.9851\n",
      "Epoch 47/60\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 10.8344 - mae: 2.5627 - val_loss: 52.4871 - val_mae: 6.6972\n",
      "Epoch 48/60\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 9.4597 - mae: 2.4008 - val_loss: 38.1173 - val_mae: 5.5502\n",
      "Epoch 49/60\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 9.7336 - mae: 2.4371 - val_loss: 39.6737 - val_mae: 5.6962\n",
      "Epoch 50/60\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 10.2055 - mae: 2.4696 - val_loss: 32.0539 - val_mae: 5.0164\n",
      "Epoch 51/60\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 12.1845 - mae: 2.7810 - val_loss: 27.7027 - val_mae: 4.6129\n",
      "Epoch 52/60\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 11.5307 - mae: 2.6531 - val_loss: 66.2957 - val_mae: 7.6136\n",
      "Epoch 53/60\n",
      "32/32 [==============================] - ETA: 0s - loss: 12.1463 - mae: 2.7775INFO:tensorflow:Assets written to: simple_covnet_model.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: simple_covnet_model.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 4s 135ms/step - loss: 12.1463 - mae: 2.7775 - val_loss: 16.8349 - val_mae: 3.4490\n",
      "Epoch 54/60\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 55.8505 - mae: 5.5853 - val_loss: 20.6432 - val_mae: 3.5237\n",
      "Epoch 55/60\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 49.9546 - mae: 5.5848 - val_loss: 25.0463 - val_mae: 3.8788\n",
      "Epoch 56/60\n",
      "32/32 [==============================] - ETA: 0s - loss: 32.1341 - mae: 4.4699INFO:tensorflow:Assets written to: simple_covnet_model.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: simple_covnet_model.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 4s 132ms/step - loss: 32.1341 - mae: 4.4699 - val_loss: 11.6852 - val_mae: 2.6489\n",
      "Epoch 57/60\n",
      "32/32 [==============================] - 4s 113ms/step - loss: 14.7062 - mae: 3.0220 - val_loss: 12.2421 - val_mae: 2.8344\n",
      "Epoch 58/60\n",
      "32/32 [==============================] - ETA: 0s - loss: 11.3090 - mae: 2.6624INFO:tensorflow:Assets written to: simple_covnet_model.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: simple_covnet_model.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 4s 131ms/step - loss: 11.3090 - mae: 2.6624 - val_loss: 10.3818 - val_mae: 2.5629\n",
      "Epoch 59/60\n",
      "32/32 [==============================] - ETA: 0s - loss: 9.9008 - mae: 2.4887INFO:tensorflow:Assets written to: simple_covnet_model.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: simple_covnet_model.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 4s 132ms/step - loss: 9.9008 - mae: 2.4887 - val_loss: 9.9769 - val_mae: 2.5117\n",
      "Epoch 60/60\n",
      "32/32 [==============================] - 4s 115ms/step - loss: 10.3229 - mae: 2.5124 - val_loss: 12.5020 - val_mae: 2.9107\n"
     ]
    }
   ],
   "source": [
    "model.set_weights(Wsave)\n",
    "history = model.fit(train_generator,\n",
    "        epochs=60,\n",
    "        callbacks = callbacks_list,\n",
    "        validation_data=validation_generator,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T18:00:58.237018Z",
     "start_time": "2023-12-09T17:57:05.139714Z"
    }
   },
   "id": "c3534ba093b0133c"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T17:54:16.823003Z",
     "start_time": "2023-12-09T17:54:16.820003Z"
    }
   },
   "id": "ba7cfcc41a003d51"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
